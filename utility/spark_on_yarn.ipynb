{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fee17c",
   "metadata": {},
   "source": [
    "# Spark on Yarn Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec813034",
   "metadata": {},
   "source": [
    "### _sample hostname ip_\n",
    "\n",
    "node0 10.11.0.2\n",
    "\n",
    "node1 10.11.0.3\n",
    "\n",
    "node2 10.11.0.4\n",
    "\n",
    "_if not explicitly specified, the following operations are at node0_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df467a3e",
   "metadata": {},
   "source": [
    "### _SSH Configuration_\n",
    "\n",
    "yum install ssh -y\n",
    "\n",
    "ssh-keygen -t rsa -P \"\"\n",
    "\n",
    "echo \"10.11.0.2 node0\n",
    "10.11.0.3 node1\n",
    "10.11.0.4 node2\" >> /etc/hosts;\n",
    "\n",
    "ssh-copy-id -i ~/.ssh/id_rsa.pub root@node0\n",
    "\n",
    "ssh-copy-id -i ~/.ssh/id_rsa.pub root@node1\n",
    "\n",
    "ssh-copy-id -i ~/.ssh/id_rsa.pub root@node2\n",
    "\n",
    "_if no password set for root user:_\n",
    "\n",
    "_copy id_rsa.pub of root@node0 and paste to the corresponding file under .ssh/ in node1 and node1 manually_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eef653",
   "metadata": {},
   "source": [
    "### _Install JDK_\n",
    "\n",
    "wget 'https://repo.huaweicloud.com/java/jdk/8u202-b08/jdk-8u202-linux-x64.rpm' \n",
    "\n",
    "yum install jdk-8u202-linux-x64.rpm -y \n",
    "\n",
    "echo \"export JAVA_HOME=/usr/java/jdk1.8.0_202-amd64/PATH=\\\\$PATH:\\$JAVA_HOME/bin\" > /etc/profile.d/jdk.sh\n",
    "\n",
    "source /etc/profile.d/jdk.sh\n",
    "\n",
    "scp /etc/profile.d/jdk.sh node1:/etc/profile.d/jdk.sh\n",
    "\n",
    "scp /etc/profile.d/jdk.sh node2:/etc/profile.d/jdk.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3573375",
   "metadata": {},
   "source": [
    "_if machines are in ARM architecture:_\n",
    "\n",
    "wget https://repo.huaweicloud.com/java/jdk/8u202-b08/jdk-8u202-linux-arm64-vfp-hflt.tar.gz\n",
    "\n",
    "tar -xvf jdk-8u202-linux-arm64-vfp-hflt.tar.gz\n",
    "\n",
    "mv jdk1.8.0_202 /usr/java\n",
    "\n",
    "echo \"export JAVA_HOME=/usr/java/jdk1.8.0_202/PATH=\\\\$PATH:\\$JAVA_HOME/bin\" > /etc/profile.d/jdk.sh\n",
    "\n",
    "source /etc/profile.d/jdk.sh\n",
    "\n",
    "scp /etc/profile.d/jdk.sh node1:/etc/profile.d/jdk.sh\n",
    "\n",
    "scp /etc/profile.d/jdk.sh node2:/etc/profile.d/jdk.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a56be",
   "metadata": {},
   "source": [
    "### Download Hadoop and Spark\n",
    "\n",
    "mkdir -p /data/big-data-app/res\n",
    "\n",
    "cd /data/big-data-app/res\n",
    "\n",
    "wget https://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-3.1.3/spark-3.1.3-bin-hadoop2.7.tgz\n",
    "\n",
    "wget https://archive.apache.org/dist/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz\n",
    "\n",
    "tar -xvf spark-3.1.3-bin-hadoop2.7.tgz\n",
    "\n",
    "mv spark-3.1.3-bin-hadoop2.7 ../spark3\n",
    "\n",
    "tar -xvf hadoop-2.7.7.tar.gz\n",
    "\n",
    "mv hadoop-2.7.7.tar.gz ../hadoop2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a500e75e",
   "metadata": {},
   "source": [
    "### _HDFS_\n",
    "\n",
    "yum install -y ssh rsync\n",
    "\n",
    "echo \"export HADOOP_HOME=/data/big-data-app/hadoop2\n",
    "export HADOOP_INSTALL=\\\\$HADOOP_HOME\n",
    "export HADOOP_MAPRED_HOME=\\\\$HADOOP_HOME\n",
    "export HADOOP_HDFS_HOME=\\\\$HADOOP_HOME\n",
    "export HADOOP_COMMON_HOME=\\\\$HADOOP_HOME\n",
    "export HADOOP_CONF_DIR=\\\\$HADOOP_HOME/etc/hadoop\n",
    "export YARN_HOME=\\\\$HADOOP_HOME\n",
    "export YARN_CONF_DIR=\\\\$HADOOP_HOME/etc/hadoop\n",
    "export PATH=\\\\$PATH:\\\\$HADOOP_HOME/sbin:\\\\$HADOOP_HOME/bin\" > /etc/profile.d/hadoop.sh\n",
    "\n",
    "source /etc/profile.d/hadoop.sh\n",
    "\n",
    "scp /etc/profile.d/hadoop.sh node1:/etc/profile.d/hadoop.sh\n",
    "\n",
    "scp /etc/profile.d/hadoop.sh node2:/etc/profile.d/hadoop.sh\n",
    "\n",
    "echo \"SPARK_HOME=/data/big-data-app/spark3\n",
    "PATH=\\$SPARK_HOME/bin:\\\\$PATH\" > /etc/profile.d/spark.sh\n",
    "\n",
    "source /etc/profile.d/spark.sh\n",
    "\n",
    "scp /etc/profile.d/spark.sh node1:/etc/profile.d/spark.sh\n",
    "\n",
    "scp /etc/profile.d/spark.sh node2:/etc/profile.d/spark.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7a191b",
   "metadata": {},
   "source": [
    "### Hadoop Configuration\n",
    "\n",
    "cd $HADOOP_CONF_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457932e7",
   "metadata": {},
   "source": [
    "vi capacity-scheduler.xml\n",
    "\n",
    "_replace_\n",
    "```xml\n",
    "<!-- DefaultResourceCalculator only uses Memory -->\n",
    "<property>\n",
    "    <name>yarn.scheduler.capacity.resource-calculator</name>\n",
    "    <value>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</value>\n",
    "</property>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ddf2b2",
   "metadata": {},
   "source": [
    "vi core-site.xml\n",
    "\n",
    "_replace_\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://node0:9000</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>hadoop.tmp.dir</name>\n",
    "        <value>/data/hadoop</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def57ace",
   "metadata": {},
   "source": [
    "vi hdfs-site.xml\n",
    "\n",
    "_replace_\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>2</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>dfs.namenode.secondary.http-address</name>\n",
    "        <value>node0:50090</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>dfs.namenode.secondary.https-address</name>\n",
    "        <value>node0:50091</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>dfs.permissions</name>\n",
    "        <value>false</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0098233",
   "metadata": {},
   "source": [
    "vi mapred-site.xml\n",
    "\n",
    "_replace_\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>mapreduce.framework.name</name>\n",
    "        <value>yarn</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d06290",
   "metadata": {},
   "source": [
    "_on every node_\n",
    "\n",
    "cp \\\\$SPARK_HOME/yarn/spark-3.1.3-yarn-shuffle.jar \\\\$HADOOP_HOME/share/hadoop/yarn/lib/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e1f3b9",
   "metadata": {},
   "source": [
    "vi yarn-site.xml\n",
    "\n",
    "_replace_\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>yarn.resourcemanager.hostname</name>\n",
    "        <value>node0</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.vmem-check-enabled</name>\n",
    "        <value>false</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "        <value>32768</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>yarn.scheduler.maximum-allocation-mb</name>\n",
    "        <value>16384</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.aux-services</name>\n",
    "        <value>mapreduce_shuffle,spark_shuffle</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>\n",
    "        <value>org.apache.spark.network.yarn.YarnShuffleService</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>spark.shuffle.service.port</name>\n",
    "        <value>7337</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5bfbe",
   "metadata": {},
   "source": [
    "vi slaves\n",
    "\n",
    "_replace_\n",
    "\n",
    "```xml\n",
    "node1\n",
    "node2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a727a6b",
   "metadata": {},
   "source": [
    "### _Spark Configuration_\n",
    "\n",
    "echo \"node1\n",
    "node2\" > $SPARK_HOME/conf/workers\n",
    "\n",
    "echo \"spark.eventLog.enabled true\n",
    "spark.eventLog.compress true\n",
    "spark.eventLog.dir hdfs:///logs\n",
    "spark.yarn.historyServer.address node0:18080\n",
    "\" >> $SPARK_HOME/conf/spark-default.conf\n",
    "\n",
    "export SPARK_HISTORY_OPTS=\"-Dspark.history.ui.port=18080 \\\n",
    "-Dspark.history.retainedApplications=3 \\\n",
    "-Dspark.history.fs.logDirectory=hdfs://node0:9000/logs\"\n",
    "\n",
    "cd $SPARK_HOME/sbin\n",
    "\n",
    "sh start-history-server.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888f2748",
   "metadata": {},
   "source": [
    "#Distribute resource\n",
    "\n",
    "rsync -arv /data/big-data-app node1:/data/big-data-app\n",
    "\n",
    "rsync -arv /data/big-data-app node2:/data/big-data-app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea532be9",
   "metadata": {},
   "source": [
    "### _Format namenode_\n",
    "\n",
    "cd $HADOOP_HOME/bin\n",
    "./hdfs namenode -format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187dff1e",
   "metadata": {},
   "source": [
    "### _Start service_\n",
    "\n",
    "cd $HADOOP_HOME/bin\n",
    "start-all.sh\n",
    "\n",
    "_on every node_\n",
    "\n",
    "jps\n",
    "\n",
    "_expected instances_\n",
    "\n",
    "```xml\n",
    "node0:\n",
    "NameNode \n",
    "SecondaryNode \n",
    "ResourceManager\n",
    "\n",
    "node1:\n",
    "DataNode \n",
    "NodeManager\n",
    "\n",
    "node2:\n",
    "DataNode \n",
    "NodeManager\n",
    "```\n",
    "_URLs_\n",
    "\n",
    "```xml\n",
    "HDFS URL http://node0:50070\n",
    "YARN URL http://node0:8088\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6888b577",
   "metadata": {},
   "source": [
    "### _Common Problem_\n",
    "\n",
    "_NodeManagers fail to initialize (caused by Hadoop can't find spark-3.1.3-yarn-shuffle.jar):_\n",
    "\n",
    "cp \\\\${SPARK_HOME}/yarn/spark-3.1.3-yarn-shuffle.jar ${HADOOP_HOME}/share/hadoop/yarn/lib/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355b058",
   "metadata": {},
   "source": [
    "### _Spark Submit_\n",
    "\n",
    "_example 1_\n",
    "\n",
    "bin/spark-submit examples/src/main/python/pi.py\n",
    "\n",
    "_example 2_\n",
    "\n",
    "./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster examples/jars/spark-examples*.jar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
